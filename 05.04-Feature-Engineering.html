
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Feature Engineering &#8212; Data Mining</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '05.04-Feature-Engineering';</script>
    <link rel="icon" href="_static/fum-logo.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Naive Bayes Classification" href="05.05-Naive-Bayes.html" />
    <link rel="prev" title="Hyperparameters and Model Validation" href="05.03-Hyperparameters-and-Model-Validation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fum-cs-logo.png" class="logo__image only-light" alt="Data Mining - Home"/>
    <script>document.write(`<img src="_static/fum-cs-logo.png" class="logo__image only-dark" alt="Data Mining - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to Data Mining Course
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundation from the Python Data Science Handbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-Foundation-PDSH.html">Part 1: Foundation from the Python Data Science Handbook</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="05.00-Machine-Learning.html">Part 2: Machine Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="05.01-What-Is-Machine-Learning.html">What Is Machine Learning?</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.02-Introducing-Scikit-Learn.html">Introducing Scikit-Learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.03-Hyperparameters-and-Model-Validation.html">Hyperparameters and Model Validation</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Feature Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.05-Naive-Bayes.html">Naive Bayes Classification</a></li>



<li class="toctree-l2"><a class="reference internal" href="05.05-Bayesian-Decision-Theory.html">Bayesian Decision Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="Covariance-Matrix.html">Understanding the Covariance Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="MLE-intro.html">Maximum Likelihood Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="Mahalanobis-Distance.html">Mahalanobis Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="kNN-Classification-Evaluation.html">k-Nearest Neighbors and Classification Evaluation Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="Voronoi-Diagrams-and-Classification2Clustering.html">Voronoi Diagrams and Their Connections to Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.11-Clustering.html">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="K-Means-Clustering.html">k-means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.12-Gaussian-Mixtures-part1.html">Gaussian Mixture Models, Part 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.12-Gaussian-Mixtures-part2.html">Gaussian Mixture Model, Part 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.06-Linear-Regression-part1.html">Linear Regression, Part 1</a></li>



<li class="toctree-l2"><a class="reference internal" href="05.06-Linear-Regression-part2.html">Linear Regression, Part 2</a></li>



<li class="toctree-l2"><a class="reference internal" href="Clustering-Validation-Metrics.html">Clustering Validation Metrics</a></li>

<li class="toctree-l2"><a class="reference internal" href="05.09-Principal-Component-Analysis-part1.html">Principal Component Analysis, Part 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.09-Principal-Component-Analysis-part2.html">Principal Component Analysis, Part 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.07-Support-Vector-Machines.html">Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.08-Random-Forests.html">Decision Trees and Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.10-Manifold-Learning.html">Manifold Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.13-Kernel-Density-Estimation.html">Kernel Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="05.14-Image-Features.html">Application: A Face Detection Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="App-Mathematics-and-Machine-Learning.html">Appendix: Mathematics and Machine Learning</a></li>

</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/fum-cs/data-mining/blob/main/docs/05.04-Feature-Engineering.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/fum-cs/data-mining" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/fum-cs/data-mining/issues/new?title=Issue%20on%20page%20%2F05.04-Feature-Engineering.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/05.04-Feature-Engineering.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Feature Engineering</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-features">Categorical Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-features">Text Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-features">Image Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derived-features">Derived Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imputation-of-missing-data">Imputation of Missing Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-pipelines">Feature Pipelines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feaure-engineering">Feaure Engineering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imputation">Imputation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-completion">Image Completion</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-representation">Sparse Representation</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="feature-engineering">
<h1>Feature Engineering<a class="headerlink" href="#feature-engineering" title="Link to this heading">#</a></h1>
<p>The previous chapters outlined the fundamental ideas of machine learning, but all of the examples assumed that you have numerical data in a tidy, <code class="docutils literal notranslate"><span class="pre">[n_samples,</span> <span class="pre">n_features]</span></code> format.
In the real world, data rarely comes in such a form.
With this in mind, one of the more important steps in using machine learning in practice is <em>feature engineering</em>: that is, taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.</p>
<p>In this chapter, we will cover a few common examples of feature engineering tasks: we’ll look at features for representing categorical data, text, and images.
Additionally, we will discuss derived features for increasing model complexity and imputation of missing data.
This process is commonly referred to as vectorization, as it involves converting arbitrary data into well-behaved vectors.</p>
<section id="categorical-features">
<h2>Categorical Features<a class="headerlink" href="#categorical-features" title="Link to this heading">#</a></h2>
<p>One common type of nonnumerical data is <em>categorical</em> data.
For example, imagine you are exploring some data on housing prices, and along with numerical features like “price” and “rooms,” you also have “neighborhood” information.
For example, your data might look something like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="mi">850000</span><span class="p">,</span> <span class="s1">&#39;rooms&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;neighborhood&#39;</span><span class="p">:</span> <span class="s1">&#39;Queen Anne&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="mi">700000</span><span class="p">,</span> <span class="s1">&#39;rooms&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;neighborhood&#39;</span><span class="p">:</span> <span class="s1">&#39;Fremont&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="mi">650000</span><span class="p">,</span> <span class="s1">&#39;rooms&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;neighborhood&#39;</span><span class="p">:</span> <span class="s1">&#39;Wallingford&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="mi">600000</span><span class="p">,</span> <span class="s1">&#39;rooms&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;neighborhood&#39;</span><span class="p">:</span> <span class="s1">&#39;Fremont&#39;</span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>You might be tempted to encode this data with a straightforward numerical mapping:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;Queen Anne&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;Fremont&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;Wallingford&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>But it turns out that this is not generally a useful approach in Scikit-Learn. The package’s models make the fundamental assumption that numerical features reflect algebraic quantities, so such a mapping would imply, for example, that <em>Queen Anne &lt; Fremont &lt; Wallingford</em>, or even that <em>Wallingford–Queen Anne = Fremont</em>, which (niche demographic jokes aside) does not make much sense.</p>
<p>In this case, one proven technique is to use <em>one-hot encoding</em>, which effectively creates extra columns indicating the presence or absence of a category with a value of 1 or 0, respectively.
When your data takes the form of a list of dictionaries, Scikit-Learn’s <code class="docutils literal notranslate"><span class="pre">DictVectorizer</span></code> will do this for you:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[     0,      1,      0, 850000,      4],
       [     1,      0,      0, 700000,      3],
       [     0,      0,      1, 650000,      3],
       [     1,      0,      0, 600000,      2]])
</pre></div>
</div>
</div>
</div>
<p>Notice that the <code class="docutils literal notranslate"><span class="pre">neighborhood</span></code> column has been expanded into three separate columns representing the three neighborhood labels, and that each row has a 1 in the column associated with its neighborhood.
With these categorical features thus encoded, you can proceed as normal with fitting a Scikit-Learn model.</p>
<p>To see the meaning of each column, you can inspect the feature names:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;neighborhood=Fremont&#39;, &#39;neighborhood=Queen Anne&#39;,
       &#39;neighborhood=Wallingford&#39;, &#39;price&#39;, &#39;rooms&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>There is one clear disadvantage of this approach: if your category has many possible values, this can <em>greatly</em> increase the size of your dataset.
However, because the encoded data contains mostly zeros, a sparse output can be a very efficient solution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vec</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;4x5 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;
	with 12 stored elements in Compressed Sparse Row format&gt;
</pre></div>
</div>
</div>
</div>
<p>Nearly all of the Scikit-Learn estimators accept such sparse inputs when fitting and evaluating models. <code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.OneHotEncoder</span></code> and <code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.FeatureHasher</span></code> are two additional tools that Scikit-Learn includes to support this type of encoding.</p>
</section>
<section id="text-features">
<h2>Text Features<a class="headerlink" href="#text-features" title="Link to this heading">#</a></h2>
<p>Another common need in feature engineering is to convert text to a set of representative numerical values.
For example, most automatic mining of social media data relies on some form of encoding the text as numbers.
One of the simplest methods of encoding this type of data is by <em>word counts</em>: you take each snippet of text, count the occurrences of each word within it, and put the results in a table.</p>
<p>For example, consider the following set of three phrases:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;problem of evil&#39;</span><span class="p">,</span>
          <span class="s1">&#39;evil queen&#39;</span><span class="p">,</span>
          <span class="s1">&#39;horizon problem&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>For a vectorization of this data based on word count, we could construct individual columns representing the words “problem,” “of,” “evil,” and so on.
While doing this by hand would be possible for this simple example, the tedium can be avoided by using Scikit-Learn’s <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;3x5 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;
	with 7 stored elements in Compressed Sparse Row format&gt;
</pre></div>
</div>
</div>
</div>
<p>The result is a sparse matrix recording the number of times each word appears; it is easier to inspect if we convert this to a <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> with labeled columns:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>evil</th>
      <th>horizon</th>
      <th>of</th>
      <th>problem</th>
      <th>queen</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>There are some issues with using a simple raw word count, however: it can lead to features that put too much weight on words that appear very frequently, and this can be suboptimal in some classification algorithms.
One approach to fix this is known as <em>term frequency–inverse document frequency</em> (<em>TF–IDF</em>), which weights the word counts by a measure of how often they appear in the documents.
The syntax for computing these features is similar to the previous example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>evil</th>
      <th>horizon</th>
      <th>of</th>
      <th>problem</th>
      <th>queen</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.517856</td>
      <td>0.000000</td>
      <td>0.680919</td>
      <td>0.517856</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.605349</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.795961</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.795961</td>
      <td>0.000000</td>
      <td>0.605349</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>For an example of using TF-IDF in a classification problem, see <a class="reference internal" href="05.05-Naive-Bayes.html"><span class="std std-doc">Naive Bayes Classification</span></a>.</p>
</section>
<section id="image-features">
<h2>Image Features<a class="headerlink" href="#image-features" title="Link to this heading">#</a></h2>
<p>Another common need is to suitably encode images for machine learning analysis.
The simplest approach is what we used for the digits data in <a class="reference internal" href="05.02-Introducing-Scikit-Learn.html"><span class="std std-doc">Introducing Scikit-Learn</span></a>: simply using the pixel values themselves.
But depending on the application, such an approach may not be optimal.</p>
<p>A comprehensive summary of feature extraction techniques for images is well beyond the scope of this chapter, but you can find excellent implementations of many of the standard approaches in the <a class="reference external" href="http://scikit-image.org">Scikit-Image project</a>.
For one example of using Scikit-Learn and Scikit-Image together, see <a class="reference internal" href="05.14-Image-Features.html"><span class="std std-doc">Feature Engineering: Working with Images</span></a>.</p>
</section>
<section id="derived-features">
<h2>Derived Features<a class="headerlink" href="#derived-features" title="Link to this heading">#</a></h2>
<p>Another useful type of feature is one that is mathematically derived from some input features.
We saw an example of this in <a class="reference internal" href="05.03-Hyperparameters-and-Model-Validation.html"><span class="std std-doc">Hyperparameters and Model Validation</span></a> when we constructed <em>polynomial features</em> from our input data.
We saw that we could convert a linear regression into a polynomial regression not by changing the model, but by transforming the input!</p>
<p>For example, this data clearly cannot be well described by a straight line (see Figure 40-1):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f1647e764096e4f57d3f2a02dbdc694df9a39d327c262bdd9199715d95df2d9c.png" src="_images/f1647e764096e4f57d3f2a02dbdc694df9a39d327c262bdd9199715d95df2d9c.png" />
</div>
</div>
<p>We can still fit a line to the data using <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> and get the optimal result, as shown in Figure 40-2:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yfit</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/50902cf2f62d6cb27429719c646aa32a33d68d9364221490972c3d90de255f9a.png" src="_images/50902cf2f62d6cb27429719c646aa32a33d68d9364221490972c3d90de255f9a.png" />
</div>
</div>
<p>But it’s clear that we need a more sophisticated model to describe the relationship between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>One approach to this is to transform the data, <strong>adding extra columns of features</strong> to drive more flexibility in the model.
For example, we can add polynomial features to the data this way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[  1.   1.   1.]
 [  2.   4.   8.]
 [  3.   9.  27.]
 [  4.  16.  64.]
 [  5.  25. 125.]]
</pre></div>
</div>
</div>
</div>
<p>The derived feature matrix has one column representing <span class="math notranslate nohighlight">\(x\)</span>, a second column representing <span class="math notranslate nohighlight">\(x^2\)</span>, and a third column representing <span class="math notranslate nohighlight">\(x^3\)</span>.
Computing a linear regression on this expanded input gives a much closer fit to our data, as you can see in Figure 40-3:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yfit</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fc41ffd248b2610217488d1d69f587262cb1156c3fcc660edbdf4dd1eb71108c.png" src="_images/fc41ffd248b2610217488d1d69f587262cb1156c3fcc660edbdf4dd1eb71108c.png" />
</div>
</div>
<p>This idea of improving a model not by changing the model, but by transforming the inputs, is fundamental to many of the more powerful machine learning methods.
We’ll explore this idea further in <a class="reference internal" href="#05.06-Linear-Regression.ipynb"><span class="xref myst">Linear Regression</span></a> in the context of <em>basis function regression</em>.
More generally, this is one motivational path to the powerful set of techniques known as <em>kernel methods</em>, which we will explore in <a class="reference internal" href="05.07-Support-Vector-Machines.html"><span class="std std-doc"> Support Vector Machines</span></a>.</p>
</section>
<section id="imputation-of-missing-data">
<h2>Imputation of Missing Data<a class="headerlink" href="#imputation-of-missing-data" title="Link to this heading">#</a></h2>
<p>Another common need in feature engineering is handling of missing data.
We discussed the handling of missing data in <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> objects in <span class="xref myst">Handling Missing Data</span>, and saw that <code class="docutils literal notranslate"><span class="pre">NaN</span></code> is often is used to mark missing values.
For example, we might have a dataset that looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">nan</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="n">nan</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>   <span class="mi">3</span>  <span class="p">],</span>
              <span class="p">[</span> <span class="mi">3</span><span class="p">,</span>   <span class="mi">7</span><span class="p">,</span>   <span class="mi">9</span>  <span class="p">],</span>
              <span class="p">[</span> <span class="mi">3</span><span class="p">,</span>   <span class="mi">5</span><span class="p">,</span>   <span class="mi">2</span>  <span class="p">],</span>
              <span class="p">[</span> <span class="mi">4</span><span class="p">,</span>   <span class="n">nan</span><span class="p">,</span> <span class="mi">6</span>  <span class="p">],</span>
              <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>   <span class="mi">8</span><span class="p">,</span>   <span class="mi">1</span>  <span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">14</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>When applying a typical machine learning model to such data, we will need to first replace the missing values with some appropriate fill value.
This is known as <em>imputation</em> of missing values, and strategies range from simple (e.g., replacing missing values with the mean of the column) to sophisticated (e.g., using matrix completion or a robust model to handle such data).</p>
<p>The sophisticated approaches tend to be very application-specific, and we won’t dive into them here.
For a baseline imputation approach using the mean, median, or most frequent value, Scikit-Learn provides the <code class="docutils literal notranslate"><span class="pre">SimpleImputer</span></code> class:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="n">imp</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">imp</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[4.5, 0. , 3. ],
       [3. , 7. , 9. ],
       [3. , 5. , 2. ],
       [4. , 5. , 6. ],
       [8. , 8. , 1. ]])
</pre></div>
</div>
</div>
</div>
<p>We see that in the resulting data, the two missing values have been replaced with the mean of the remaining values in the column. This imputed data can then be fed directly into, for example, a <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> estimator:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([13.14869292, 14.3784627 , -1.15539732, 10.96606197, -5.33782027])
</pre></div>
</div>
</div>
</div>
</section>
<section id="feature-pipelines">
<h2>Feature Pipelines<a class="headerlink" href="#feature-pipelines" title="Link to this heading">#</a></h2>
<p>With any of the preceding examples, it can quickly become tedious to do the transformations by hand, especially if you wish to string together multiple steps.
For example, we might want a processing pipeline that looks something like this:</p>
<ol class="arabic simple">
<li><p>Impute missing values using the mean.</p></li>
<li><p>Transform features to quadratic.</p></li>
<li><p>Fit a linear regression model.</p></li>
</ol>
<p>To streamline this type of processing pipeline, Scikit-Learn provides a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> object, which can be used as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">),</span>
                      <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                      <span class="n">LinearRegression</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>This pipeline looks and acts like a standard Scikit-Learn object, and will apply all the specified steps to any input data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># X with missing values, from above</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[14 16 -1  8 -5]
[14. 16. -1.  8. -5.]
</pre></div>
</div>
</div>
</div>
<p>All the steps of the model are applied automatically.
Notice that for simplicity, in this demonstration we’ve applied the model to the data it was trained on; this is why it was able to perfectly predict the result (refer back to <a class="reference internal" href="05.03-Hyperparameters-and-Model-Validation.html"><span class="std std-doc">Hyperparameters and Model Validation</span></a> for further discussion of this).</p>
<p>For some examples of Scikit-Learn pipelines in action, see the following chapter on naive Bayes classification, as well as <a class="reference internal" href="#05.06-Linear-Regression.ipynb"><span class="xref myst">Linear Regression</span></a> and <a class="reference internal" href="05.07-Support-Vector-Machines.html"><span class="std std-doc"> Support Vector Machines</span></a>.</p>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<section id="feaure-engineering">
<h3>Feaure Engineering<a class="headerlink" href="#feaure-engineering" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><span id="id1">[<a class="reference internal" href="intro.html#id11" title="Sergios Theodoridis and Konstantinos Koutroumbas. Pattern Recognition, Fourth Edition. Academic Press, Inc., USA, 4th edition, 2008. ISBN 1597492728. URL: https://uupload.ir/view/bmf1_pattern_recognition-sergio_theodoridis-4th_edition.pdf.">TK08</a>]</span> has 3 chapters about feature selection and feature generation</p>
<ul>
<li><p>7.3.1 Fourier Features, 7.3.2 Chain Codes</p></li>
<li><p>Using Pattern Matching for Tiling and Packing Problems <span id="id2">[<a class="reference internal" href="intro.html#id93" title="M. Amintoosi, H. SadoghiYazdi, M.Fathy, and R. Monsefi. Using pattern matching for tiling and packing problems. European Journal of Operational Research, 183:950-960, 2007. Indexed by DBLP and SCOPUS.">ASMFathyM07</a>]</span></p></li>
</ul>
</li>
<li><p><span id="id3">[<a class="reference internal" href="intro.html#id12" title="Alice Zheng and Amanda Casari. Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. O'Reilly Media, Inc., 1st edition, 2018. ISBN 1491953241. URL: https://www.repath.in/gallery/feature_engineering_for_machine_learning.pdf.">ZC18</a>]</span> is devoted to feature engineering</p></li>
<li><p>Natural Language Processing with Python <span id="id4">[<a class="reference internal" href="intro.html#id13" title="Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python. O'Reilly Media, Inc., 1st edition, 2009. ISBN 0596516495. URL: https://tjzhifei.github.io/resources/NLTK.pdf.">BKL09</a>]</span></p></li>
<li><p>Some Papers:</p>
<ul>
<li><p>Application of the Neural Network-based Machine Learning Method to Classify Scientific Articles <span id="id5">[<a class="reference internal" href="intro.html#id10" title="Masood Ghayoomi and Maryam Mousavian. Application of the neural network-based machine learning method to classify scientific articles. Iranian Journal of Information Processing and Management, 37(4):1244-1217, 2022. URL: https://jipm.irandoc.ac.ir/article_699699.html, arXiv:https://jipm.irandoc.ac.ir/article_699699_499edcec59b5f54fe0ce422abd7e629b.pdf, doi:10.35050/JIPM010.2022.008.">GM22</a>]</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="imputation">
<h3>Imputation<a class="headerlink" href="#imputation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://www.theanalysisfactor.com/seven-ways-to-make-up-data-common-methods-to-imputing-missing-data/">Seven Ways to Make up Data: Common Methods to Imputing Missing Data</a></p></li>
<li><p><a class="reference external" href="https://bamos.github.io/2016/08/09/deep-completion/">Image Completion with Deep Learning in TensorFlow</a>:</p></li>
</ul>
<blockquote>
<div><p>The key relationship between images and statistics is that we can interpret images as samples from a high-dimensional probability distribution.
<img alt="" src="_images/normal-2d-max.png" /></p>
</div></blockquote>
<section id="image-completion">
<h4>Image Completion<a class="headerlink" href="#image-completion" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>2003: <a class="reference external" href="https://cs.brown.edu/courses/cs129/2011/asgn/proj3/resources/GraphcutTextures.pdf">Graphcut Textures: Image and Video Synthesis Using Graph Cuts</a></p>
<ul>
<li><p>MSc. project <span id="id6">[<a class="reference internal" href="intro.html#id17" title="Fateme sadat Hoseini. Graph minimum cut. Master's thesis, Hakim Sabzevari University, Faculty of Mathematics and Computer Science, October 2021. برش کمینه در گراف. URL: http://hcloud.hsu.ac.ir/index.php/s/OtehB4LUQFv8cla.">sH21</a>]</span></p></li>
</ul>
</li>
<li><p>2004: <a class="reference external" href="https://cs.brown.edu/courses/cs129/2011/asgn/proj3/">Patch-based image completion</a></p></li>
<li><p>2021: <a class="reference external" href="https://github.com/raywzy/ICT">High-Fidelity Pluralistic Image Completion with Transformers</a></p></li>
<li><p>2024: <a class="reference external" href="https://arxiv.org/abs/2212.06310">Structure-Guided Image Completion with Image-level and Object-level Semantic Discriminators</a></p></li>
</ul>
</section>
</section>
<section id="sparse-representation">
<h3>Sparse Representation<a class="headerlink" href="#sparse-representation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Wiki of <a class="reference external" href="https://en.wikipedia.org/wiki/Sparse_approximation">Sparse approximation</a></p></li>
<li><p><a class="reference external" href="https://scholar.google.com/citations?user=5H-SuMcAAAAJ&amp;amp;hl=en">Massoud Babaie-Zadeh</a></p></li>
<li><p><a class="reference external" href="https://web.stanford.edu/class/cme324/saad.pdf">Iterative Methods for Sparse Linear Systems</a>, Yousef Saad</p></li>
<li><p>Some Msc. projects: <span id="id7">[<a class="reference internal" href="intro.html#id18" title="Mahboube Bakhshali. The subspace pursuit method in sparseoptimization. Master's thesis, Hakim Sabzevari University, Faculty of Mathematics and Computer Science, September 2018. روش جستجوی زیرفضا در بهینه سازی تنک. URL: http://hcloud.hsu.ac.ir/index.php/s/jacmnZiPfNFpYfk.">Bak18</a>]</span>, <span id="id8">[<a class="reference internal" href="intro.html#id19" title="Zahra Asaadi. Parallel version of the conjugategradient method in distributed andshared environments. Master's thesis, Hakim Sabzevari University, Faculty of Mathematics and Computer Science, October 2021. نسخه موازی روش گرادیان مزدوج در محیط های توزیعی و اشتراکی. URL: http://hcloud.hsu.ac.ir/index.php/s/B4oT53TfOknyNEg.">Asa21</a>]</span>, <span id="id9">[<a class="reference internal" href="intro.html#id16" title="Alale Asaran. Super resolution via sparse representation. Master's thesis, Hakim Sabzevari University, Faculty of Mathematics and Computer Science, Winter 2016. فراتفکیک پذیری با نمایش تنک. URL: http://hcloud.hsu.ac.ir/index.php/s/W9ImIzeV6C1mqZo.">Asa16</a>]</span></p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="05.03-Hyperparameters-and-Model-Validation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Hyperparameters and Model Validation</p>
      </div>
    </a>
    <a class="right-next"
       href="05.05-Naive-Bayes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Naive Bayes Classification</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-features">Categorical Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-features">Text Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-features">Image Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derived-features">Derived Features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imputation-of-missing-data">Imputation of Missing Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-pipelines">Feature Pipelines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feaure-engineering">Feaure Engineering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imputation">Imputation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-completion">Image Completion</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-representation">Sparse Representation</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mahmood Amintoosi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025. CC0 Licensed - Computer Science Dept., Ferdowsi University of Mashhad.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>