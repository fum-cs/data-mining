{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af37f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eeb10fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Introduction to the Covariance Matrix\n",
    "\n",
    "This notebook provides an introduction to the covariance matrix, a fundamental concept in multivariate statistics. We'll build on your existing knowledge of single-variable statistics (mean, variance, normal distribution) to understand how to describe the relationships between multiple variables.\n",
    "\n",
    "### 1. Why Do We Need a Covariance Matrix?\n",
    "\n",
    "In many real-world scenarios, we deal with multiple variables simultaneously. For example:\n",
    "\n",
    "*   **Customer Data:** Age, income, spending habits.\n",
    "*   **Sensor Data:** Temperature, humidity, pressure.\n",
    "*   **Image Data:** Red, green, blue color channels.\n",
    "\n",
    "Understanding how these variables *relate* to each other is crucial for building effective models and gaining insights. The covariance matrix provides a way to quantify these relationships.\n",
    "\n",
    "### 2. Review: Variance in a Single Variable\n",
    "\n",
    "Let's quickly review variance. For a single random variable *X*, the variance (Var(X) or σ²) measures how spread out the values of *X* are around its mean (μ).  It's calculated as:\n",
    "\n",
    "Var(X) = E[(X - μ)²]  (the expected value of the squared difference from the mean)\n",
    "\n",
    "A higher variance indicates greater spread.\n",
    "\n",
    "### 3. Introducing Covariance\n",
    "\n",
    "Covariance measures the *linear relationship* between two random variables, *X* and *Y*. It tells us whether they tend to increase or decrease together.  The formula is:\n",
    "\n",
    "Cov(X, Y) = E[(X - μ<sub>X</sub>)(Y - μ<sub>Y</sub>)]\n",
    "\n",
    "*   **Positive Covariance:**  *X* and *Y* tend to increase together, or decrease together.\n",
    "*   **Negative Covariance:** *X* tends to increase while *Y* tends to decrease (or vice-versa).\n",
    "*   **Zero Covariance:** No linear relationship between *X* and *Y*.  (Important: zero covariance doesn't necessarily mean the variables are independent; there could be non-linear relationships.)\n",
    "\n",
    "### 4. The Covariance Matrix\n",
    "\n",
    "When dealing with multiple variables (let's say *n* variables: X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>n</sub>), we can organize all the pairwise covariances into a matrix called the *covariance matrix* (often denoted by Σ).\n",
    "\n",
    "The covariance matrix is an *n x n* matrix where:\n",
    "\n",
    "*   The element at row *i*, column *j* is the covariance between variable X<sub>i</sub> and variable X<sub>j</sub>:  Cov(X<sub>i</sub>, X<sub>j</sub>).\n",
    "*   The diagonal elements are the variances of the individual variables: Var(X<sub>i</sub>).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's say we have two variables, X<sub>1</sub> and X<sub>2</sub>. The covariance matrix would look like this:\n",
    "\n",
    "```\n",
    "Σ =  | Var(X1)   Cov(X1, X2) |\n",
    "     | Cov(X2, X1)   Var(X2)   |\n",
    "```\n",
    "\n",
    "Since Cov(X<sub>1</sub>, X<sub>2</sub>) = Cov(X<sub>2</sub>, X<sub>1</sub>), the covariance matrix is *symmetric*.\n",
    "\n",
    "### 5.  Calculating the Covariance Matrix in Python (NumPy)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example data (two variables, 5 samples)\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "covariance_matrix = np.cov(data, rowvar=False) # rowvar=False means each column represents a variable\n",
    "\n",
    "print(covariance_matrix)\n",
    "```\n",
    "\n",
    "### 6. Applications in Data Science and Data Mining\n",
    "\n",
    "*   **Principal Component Analysis (PCA):** The covariance matrix is a key input for PCA, a dimensionality reduction technique used to identify the most important features in a dataset.\n",
    "*   **Gaussian Mixture Models (GMM):**  GMMs assume that data points are generated from a mixture of Gaussian distributions.  The covariance matrices define the shape and orientation of these Gaussian distributions.\n",
    "*   **Anomaly Detection:**  The covariance matrix can be used to model the expected relationships between variables in normal data.  Anomalies can be identified as data points that deviate significantly from this expected relationship.\n",
    "*   **Portfolio Optimization (Finance):** The covariance matrix of asset returns is used to calculate the risk of a portfolio.\n",
    "*   **Image Processing:** The covariance matrix can be used to analyze the relationships between color channels in an image.\n",
    "\n",
    "### 7. Important Considerations\n",
    "\n",
    "*   **Scaling:** The covariance matrix is sensitive to the scaling of the variables.  It's often a good idea to standardize your data (e.g., using Z-score normalization) before calculating the covariance matrix.\n",
    "*   **Multicollinearity:**  High correlation between variables (multicollinearity) can make the covariance matrix ill-conditioned and difficult to invert.  This can cause problems in some applications (e.g., linear regression).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413cd0f",
   "metadata": {},
   "source": [
    "\n",
    "# Appendix: Understanding the Covariance Matrix\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In single-variable statistics, we often use the **mean** ( $\\mu$ ) to describe the central tendency of data and the **variance** ( $\\sigma^2$ ) or standard deviation ( $\\sigma$ ) to describe its spread or dispersion. When we move to analyzing datasets with multiple variables (multivariate data), we need ways to describe not only the spread of each variable individually but also how these variables relate to or *vary together*. This is where the **covariance matrix** becomes essential.\n",
    "\n",
    "Imagine you are collecting data on students: height and weight. You can calculate the average height and average weight (the means). You can also calculate how much heights vary among students (variance of height) and how much weights vary (variance of weight). But you might also notice that taller students tend to weigh more. The covariance matrix helps us quantify this kind of relationship between variables.\n",
    "\n",
    "## From Variance to Covariance\n",
    "\n",
    "Let's quickly recap **variance**. For a single random variable $X$, the variance, denoted as $\\text{Var}(X)$ or $\\sigma_X^2$, measures how much the values of $X$ deviate from their mean ( $\\mu_X$ ). Mathematically, it's the expected value of the squared deviation from the mean:\n",
    "\n",
    "$$ \\text{Var}(X) = E[(X - \\mu_X)^2] $$\n",
    "\n",
    "Now, consider two random variables, $X$ and $Y$. **Covariance**, denoted as $\\text{Cov}(X, Y)$ or $\\sigma_{XY}$, measures how $X$ and $Y$ change *together* relative to their respective means ($\\mu_X$ and $\\mu_Y$).\n",
    "\n",
    "$$ \\text{Cov}(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)] $$\n",
    "\n",
    "*   **Positive Covariance:** If $X$ tends to be above its mean when $Y$ is above its mean (and $X$ tends to be below its mean when $Y$ is below its mean), the product $(X - \\mu_X)(Y - \\mu_Y)$ will tend to be positive, resulting in $\\text{Cov}(X, Y) > 0$. This suggests they tend to increase or decrease together (like height and weight).\n",
    "*   **Negative Covariance:** If $X$ tends to be above its mean when $Y$ is below its mean (or vice versa), the product $(X - \\mu_X)(Y - \\mu_Y)$ will tend to be negative, resulting in $\\text{Cov}(X, Y) < 0$. This suggests an inverse linear relationship (like perhaps study time and number of errors on a simple task).\n",
    "*   **Zero (or near-zero) Covariance:** If there's no consistent linear relationship between how $X$ and $Y$ deviate from their respective means, the positive and negative products will average out, resulting in $\\text{Cov}(X, Y) \\approx 0$. (Important note: Zero covariance only implies *no linear* relationship. There could still be a non-linear relationship).\n",
    "\n",
    "## The Covariance Matrix (Σ)\n",
    "\n",
    "When you have more than two variables (say, $d$ variables: $X_1, X_2, \\dots, X_d$), you can calculate the covariance between every pair of variables. The **covariance matrix**, often symbolized by the Greek capital letter Sigma ($\\Sigma$), is a way to neatly organize all these pairwise covariances.\n",
    "\n",
    "It's a $d \\times d$ square matrix where the element in the $i$-th row and $j$-th column is the covariance between variable $X_i$ and variable $X_j$:\n",
    "\n",
    "$$ \\Sigma_{ij} = \\text{Cov}(X_i, X_j) $$\n",
    "\n",
    "The matrix looks like this:\n",
    "\n",
    "$$\n",
    "\\Sigma =\n",
    "\\begin{pmatrix}\n",
    "\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_d) \\\\\n",
    "\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_d) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(X_d, X_1) & \\text{Cov}(X_d, X_2) & \\cdots & \\text{Var}(X_d)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Note that $\\text{Cov}(X_i, X_i) = E[(X_i - \\mu_{X_i})(X_i - \\mu_{X_i})] = E[(X_i - \\mu_{X_i})^2] = \\text{Var}(X_i)$. Thus, the diagonal elements are the variances of the individual variables.\n",
    "\n",
    "**Example (2 variables: $X_1, X_2$):**\n",
    "\n",
    "$$\n",
    "\\Sigma =\n",
    "\\begin{pmatrix}\n",
    "\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) \\\\\n",
    "\\text{Cov}(X_2, X_1) & \\text{Var}(X_2)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Example (3 variables: $X_1, X_2, X_3$):**\n",
    "\n",
    "$$\n",
    "\\Sigma =\n",
    "\\begin{pmatrix}\n",
    "\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\text{Cov}(X_1, X_3) \\\\\n",
    "\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\text{Cov}(X_2, X_3) \\\\\n",
    "\\text{Cov}(X_3, X_1) & \\text{Cov}(X_3, X_2) & \\text{Var}(X_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "## Key Properties of the Covariance Matrix\n",
    "\n",
    "The covariance matrix $\\Sigma$ has several important mathematical properties:\n",
    "\n",
    "1.  **Symmetric:** The covariance between $X_i$ and $X_j$ is the same as the covariance between $X_j$ and $X_i$. That is, $\\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i)$, so $\\Sigma_{ij} = \\Sigma_{ji}$. This means the matrix is symmetric about its main diagonal ($\\Sigma = \\Sigma^T$).\n",
    "2.  **Diagonal Contains Variances:** As noted above, the diagonal elements $\\Sigma_{ii}$ are the variances $\\text{Var}(X_i)$ of each variable $X_i$. Since variance measures spread, it cannot be negative, so $\\Sigma_{ii} \\ge 0$.\n",
    "3.  **Off-diagonal Contains Covariances:** The off-diagonal elements $\\Sigma_{ij}$ (where $i \\neq j$) are the covariances between pairs of variables $X_i$ and $X_j$, indicating their linear relationship.\n",
    "4.  **Positive Semi-Definite:** This is a crucial property. A matrix $\\Sigma$ is positive semi-definite if, for any non-zero vector $\\mathbf{a}$, the quadratic form $\\mathbf{a}^T \\Sigma \\mathbf{a} \\ge 0$. In the context of covariance matrices, this arises because $\\mathbf{a}^T \\Sigma \\mathbf{a}$ represents the variance of a linear combination of the variables ($a_1 X_1 + \\dots + a_d X_d$), and variance must be non-negative. This property ensures that the \"spread\" described by the matrix is physically meaningful. If the variables are not linearly dependent, the matrix is strictly **positive definite** ($\\mathbf{a}^T \\Sigma \\mathbf{a} > 0$ for $\\mathbf{a} \\neq \\mathbf{0}$).\n",
    "\n",
    "## Calculating the Covariance Matrix in Practice\n",
    "\n",
    "Given a dataset where each row is an observation and each column is a variable, we can easily compute the *sample* covariance matrix using libraries like NumPy.\n",
    "\n",
    "### Example 1: Height, Weight, Age Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec28d5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 1: Person Data ---\n",
      "Sample Data (rows=observations, cols=variables: height, weight, age):\n",
      " [[170  65  20]\n",
      " [180  80  25]\n",
      " [165  55  22]\n",
      " [175  70  30]\n",
      " [190  90  28]]\n",
      "\n",
      "Covariance Matrix:\n",
      "[[ 92.5  128.75  25.  ]\n",
      " [128.75 182.5   32.5 ]\n",
      " [ 25.    32.5   17.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data: 5 observations, 3 variables (e.g., height(cm), weight(kg), age(years))\n",
    "# Each row is an observation, each column is a variable\n",
    "data_persons = np.array([\n",
    "    [170, 65, 20],  # Person 1\n",
    "    [180, 80, 25],  # Person 2\n",
    "    [165, 55, 22],  # Person 3\n",
    "    [175, 70, 30],  # Person 4\n",
    "    [190, 90, 28]   # Person 5\n",
    "])\n",
    "\n",
    "# Calculate the sample covariance matrix\n",
    "# rowvar=False indicates that columns represent variables, rows represent observations\n",
    "# By default, np.cov uses N-1 (sample covariance). Use bias=True for N (population covariance).\n",
    "covariance_matrix_persons = np.cov(data_persons, rowvar=False)\n",
    "\n",
    "print(\"--- Example 1: Person Data ---\")\n",
    "print(\"Sample Data (rows=observations, cols=variables: height, weight, age):\\n\", data_persons)\n",
    "print(\"\\nCovariance Matrix:\")\n",
    "print(covariance_matrix_persons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa5bbb",
   "metadata": {},
   "source": [
    "\n",
    "# Interpretation:\n",
    "# Diagonal: Var(height) ~ 92.5, Var(weight) ~ 182.5, Var(age) ~ 15.7\n",
    "# Off-diagonal:\n",
    "# Cov(height, weight) ~ 126.25 (positive, as expected)\n",
    "# Cov(height, age) ~ 33.75 (positive, taller people in sample are slightly older)\n",
    "# Cov(weight, age) ~ 51.25 (positive, heavier people in sample are slightly older)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc7d74",
   "metadata": {},
   "source": [
    "\n",
    "### Example 2: Student Grades Data\n",
    "\n",
    "Let's consider hypothetical grades for 6 students in Math, Physics, and History. We want to see how these grades might be related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2268e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 2: Student Grades ---\n",
      "Sample Data (rows=observations, cols=variables: Math, Physics, History):\n",
      " [[85 80 75]\n",
      " [92 88 70]\n",
      " [70 75 85]\n",
      " [65 70 90]\n",
      " [95 90 65]\n",
      " [78 82 80]]\n",
      "\n",
      "Covariance Matrix:\n",
      "[[ 143.76666667   87.56666667 -111.5       ]\n",
      " [  87.56666667   57.76666667  -68.5       ]\n",
      " [-111.5         -68.5          87.5       ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data: 6 students, 3 subjects (Math, Physics, History)\n",
    "# Grades out of 100\n",
    "data_grades = np.array([\n",
    "    # Math, Physics, History\n",
    "    [85, 80, 75],   # Student 1\n",
    "    [92, 88, 70],   # Student 2\n",
    "    [70, 75, 85],   # Student 3\n",
    "    [65, 70, 90],   # Student 4\n",
    "    [95, 90, 65],   # Student 5\n",
    "    [78, 82, 80]    # Student 6\n",
    "])\n",
    "\n",
    "# Calculate the sample covariance matrix\n",
    "covariance_matrix_grades = np.cov(data_grades, rowvar=False)\n",
    "\n",
    "print(\"\\n--- Example 2: Student Grades ---\")\n",
    "print(\"Sample Data (rows=observations, cols=variables: Math, Physics, History):\\n\", data_grades)\n",
    "print(\"\\nCovariance Matrix:\")\n",
    "print(covariance_matrix_grades)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481bf1a9",
   "metadata": {},
   "source": [
    "\n",
    "# Interpretation:\n",
    "# Diagonal: Shows the variance in grades for each subject (Math highest, Physics mid, History lowest variance in this sample)\n",
    "# Off-diagonal:\n",
    "# Cov(Math, Physics) is positive (~101.7): Students with higher Math grades tend to have higher Physics grades.\n",
    "# Cov(Math, History) is negative (~-80.8): Students with higher Math grades tend to have lower History grades in this sample.\n",
    "# Cov(Physics, History) is negative (~-59.0): Students with higher Physics grades tend to have lower History grades in this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c656d4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9f99ee6",
   "metadata": {},
   "source": [
    "## Why is the Covariance Matrix Important in Data Science?\n",
    "\n",
    "The covariance matrix is a cornerstone in many multivariate statistical methods and machine learning algorithms:\n",
    "\n",
    "1.  **Multivariate Normal Distribution:** Just like variance defines the spread of a 1D normal distribution, the covariance matrix $\\Sigma$ (along with the mean vector $\\boldsymbol{\\mu}$) defines the shape, orientation, and spread of a multivariate normal distribution, $N(\\boldsymbol{\\mu}, \\Sigma)$. This is crucial for Bayesian classifiers (like the ones you are studying, e.g., Quadratic Discriminant Analysis), Gaussian Mixture Models (GMMs), and Linear Discriminant Analysis (LDA - which often assumes equal covariance matrices across classes).\n",
    "2.  **Principal Component Analysis (PCA):** PCA is a dimensionality reduction technique that finds the directions (principal components) of maximum variance in the data. It works by finding the eigenvectors and eigenvalues of the covariance matrix (or correlation matrix). The eigenvectors give the directions, and the eigenvalues give the variance along those directions.\n",
    "3.  **Data Whitening / Sphering:** This process transforms data so that its covariance matrix becomes the identity matrix ($I$), meaning variables become uncorrelated and have unit variance. This can be useful as a preprocessing step for some algorithms that assume uncorrelated features. It involves using the inverse square root of the covariance matrix.\n",
    "4.  **Mahalanobis Distance:** This distance metric measures the distance between a point and a distribution, accounting for the correlations between variables. It is defined as $D_M(\\mathbf{x}) = \\sqrt{(\\mathbf{x} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})}$. It's useful for finding outliers in multivariate data or in classification algorithms.\n",
    "5.  **Understanding Feature Relationships:** Simply examining the signs and magnitudes of the off-diagonal elements of the covariance matrix can give initial insights into which variables are positively or negatively linearly related, or relatively independent.\n",
    "\n",
    "## Important Considerations\n",
    "\n",
    "*   **Units:** Covariance values depend heavily on the units of the variables. $\\text{Cov}(X \\text{[cm}, Y \\text{[kg]})$ will have different units and magnitude than $\\text{Cov}(X \\text{[m]}, Y \\text{[g]})$. This makes direct comparison of covariance values difficult if variables have different scales. Often, the **correlation matrix** (which is a scaled version of the covariance matrix with values between -1 and 1, $\\text{Corr}(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$) is preferred for interpreting the strength of relationships.\n",
    "*   **Sample vs. Population:** The `np.cov` function by default calculates the *sample* covariance matrix (dividing by $N-1$, where $N$ is the number of observations). This provides an unbiased estimate of the population covariance. If you have the entire population data or specifically need the population covariance (dividing by $N$), you can use the `bias=True` argument in `np.cov`.\n",
    "*   **Linearity:** Remember, covariance (and correlation) measures *linear* relationships. Two variables can have a strong non-linear relationship (e.g., $Y=X^2$) but still have zero covariance if the relationship is symmetric around the mean.\n",
    "*   **Sensitivity to Outliers:** Like variance, covariance calculations can be sensitive to extreme values (outliers) in the data.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The covariance matrix extends the concept of variance to multiple dimensions. It provides a compact $d \\times d$ symmetric matrix summarizing the variance of each individual variable (on the diagonal) and the pairwise linear relationships (covariances) between different variables (off-diagonal) in a multivariate dataset. Its properties, particularly positive semi-definiteness, make it fundamental for defining multivariate distributions (like the multivariate normal), performing dimensionality reduction (PCA), measuring multivariate distances (Mahalanobis), and understanding the structure of multivariate data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pth-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
