{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors and Classification Evaluation Metrics\n",
    "\n",
    "This notebook covers:\n",
    "1. Review of k-Nearest Neighbors (kNN) algorithm\n",
    "2. Classification evaluation metrics: Accuracy, Precision, Recall, and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review of k-Nearest Neighbors (kNN)\n",
    "\n",
    "The k-Nearest Neighbors algorithm is a non-parametric, instance-based learning method used for classification and regression. The algorithm works by finding the k training samples closest in distance to a new sample and predicting the class label based on a majority vote.\n",
    "\n",
    "### Key Characteristics of kNN:\n",
    "\n",
    "- **Non-parametric**: The algorithm doesn't make assumptions about the underlying data distribution.\n",
    "- **Instance-based (lazy learning)**: No explicit training phase; the model simply stores the training data.\n",
    "- **Distance-based**: Predictions are made based on the distance between points.\n",
    "\n",
    "### The kNN Algorithm (as described in Duda et al.):\n",
    "\n",
    "1. Store all training samples with their class labels.\n",
    "2. For a new sample x:\n",
    "   - Calculate the distance between x and all training samples.\n",
    "   - Select the k closest training samples (k-nearest neighbors).\n",
    "   - Assign the class label based on majority voting among the k neighbors.\n",
    "\n",
    "### Distance Metrics:\n",
    "\n",
    "The most common distance metrics used in kNN are:\n",
    "\n",
    "1. **Euclidean Distance**: $d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$\n",
    "2. **Manhattan Distance**: $d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|$\n",
    "3. **Minkowski Distance**: $d(x, y) = (\\sum_{i=1}^{n} |x_i - y_i|^p)^{1/p}$\n",
    "\n",
    "Let's implement and visualize the kNN algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a simple dataset\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.2)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', s=50, alpha=0.8, edgecolors='k')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', s=100, alpha=0.3, marker='s')\n",
    "plt.title('Dataset with 3 Classes')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend(['Training Data', 'Testing Data'])\n",
    "plt.colorbar(label='Class Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implement kNN with different values of k\n",
    "k_values = [1, 3, 5, 15]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create a meshgrid for decision boundary visualization\n",
    "h = 0.02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    # Train kNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the meshgrid\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundaries\n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    axes[i].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', s=50, edgecolors='k')\n",
    "    axes[i].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', s=100, alpha=0.3, marker='s')\n",
    "    axes[i].set_title(f'kNN with k={k}')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "    \n",
    "    # Calculate and display accuracy\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    axes[i].text(0.05, 0.95, f'Accuracy: {accuracy:.2f}', transform=axes[i].transAxes, \n",
    "                 bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on kNN Behavior:\n",
    "\n",
    "1. **Effect of k**:\n",
    "   - Small k: Decision boundaries are more complex and can lead to overfitting\n",
    "   - Large k: Decision boundaries are smoother but may miss important patterns\n",
    "\n",
    "2. **Computational Complexity**:\n",
    "   - Training: O(1) - just stores the data\n",
    "   - Prediction: O(nd) where n is the number of training samples and d is the number of features\n",
    "\n",
    "3. **Curse of Dimensionality**:\n",
    "   - As the number of dimensions increases, the distance metric becomes less meaningful\n",
    "   - In high dimensions, all points tend to be equidistant from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Evaluation Metrics\n",
    "\n",
    "To evaluate the performance of classification models, we need appropriate metrics. The choice of metric depends on the problem context and the relative importance of different types of errors.\n",
    "\n",
    "### 2.1 Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that describes the performance of a classification model. For a binary classification problem, it contains:\n",
    "\n",
    "- **True Positives (TP)**: Correctly predicted positive cases\n",
    "- **False Positives (FP)**: Incorrectly predicted positive cases (Type I error)\n",
    "- **True Negatives (TN)**: Correctly predicted negative cases\n",
    "- **False Negatives (FN)**: Incorrectly predicted negative cases (Type II error)\n",
    "\n",
    "![Confusion Matrix](img/confusion_matrix.png)\n",
    "\n",
    "Let's create a binary classification problem and visualize its confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a binary classification dataset\n",
    "X_binary, y_binary = make_classification(n_samples=1000, n_features=2, n_informative=2, \n",
    "                                         n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X_binary, y_binary, \n",
    "                                                                     test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a kNN classifier\n",
    "knn_binary = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_binary.fit(X_train_bin, y_train_bin)\n",
    "y_pred_bin = knn_binary.predict(X_test_bin)\n",
    "\n",
    "# Create and display the confusion matrix\n",
    "cm = confusion_matrix(y_test_bin, y_pred_bin)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix for Binary Classification')\n",
    "plt.show()\n",
    "\n",
    "# Extract values from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP): {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Accuracy\n",
    "\n",
    "Accuracy is the ratio of correctly predicted instances to the total instances.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Pros**:\n",
    "- Simple and intuitive\n",
    "- Works well for balanced datasets\n",
    "\n",
    "**Cons**:\n",
    "- Misleading for imbalanced datasets\n",
    "- Doesn't distinguish between types of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate accuracy\n",
    "acc = accuracy_score(y_test_bin, y_pred_bin)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Manual calculation\n",
    "acc_manual = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"Manually calculated accuracy: {acc_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Precision\n",
    "\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: \"Of all instances predicted as positive, how many are actually positive?\"\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Pros**:\n",
    "- Important when the cost of false positives is high\n",
    "- Useful in information retrieval, medical diagnosis\n",
    "\n",
    "**Cons**:\n",
    "- Doesn't consider false negatives\n",
    "- Can be artificially high if the model rarely predicts the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate precision\n",
    "prec = precision_score(y_test_bin, y_pred_bin)\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "\n",
    "# Manual calculation\n",
    "prec_manual = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "print(f\"Manually calculated precision: {prec_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Recall (Sensitivity)\n",
    "\n",
    "Recall is the ratio of correctly predicted positive observations to all actual positives. It answers the question: \"Of all actual positive instances, how many did we predict correctly?\"\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Pros**:\n",
    "- Important when the cost of false negatives is high\n",
    "- Useful in medical screening, fraud detection\n",
    "\n",
    "**Cons**:\n",
    "- Doesn't consider false positives\n",
    "- Can be artificially high if the model always predicts the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate recall\n",
    "rec = recall_score(y_test_bin, y_pred_bin)\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "\n",
    "# Manual calculation\n",
    "rec_manual = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "print(f\"Manually calculated recall: {rec_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 F1-Score\n",
    "\n",
    "F1-Score is the harmonic mean of Precision and Recall. It provides a balance between precision and recall.\n",
    "\n",
    "$$\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "**Pros**:\n",
    "- Balances precision and recall\n",
    "- Works well for imbalanced datasets\n",
    "\n",
    "**Cons**:\n",
    "- May not be appropriate if one metric is more important than the other\n",
    "- Doesn't consider true negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test_bin, y_pred_bin)\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Manual calculation\n",
    "f1_manual = 2 * (prec_manual * rec_manual) / (prec_manual + rec_manual) if (prec_manual + rec_manual) > 0 else 0\n",
    "print(f\"Manually calculated F1-Score: {f1_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Precision-Recall Trade-off\n",
    "\n",
    "There is often a trade-off between precision and recall. Increasing one typically decreases the other.\n",
    "\n",
    "![Precision-Recall Trade-off](img/precision_recall_tradeoff.png)\n",
    "\n",
    "Let's visualize this trade-off by adjusting the decision threshold for a probabilistic classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a new dataset\n",
    "X_new, y_new = make_classification(n_samples=1000, n_features=2, n_informative=2, \n",
    "                                   n_redundant=0, random_state=42, weights=[0.7, 0.3])\n",
    "\n",
    "# Split the data\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, \n",
    "                                                                     test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a kNN classifier with probability estimates\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_prob = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn_prob.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Get probability estimates\n",
    "y_proba = knn_prob.predict_proba(X_test_new)[:, 1]\n",
    "\n",
    "# Calculate precision and recall for different thresholds\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_proba >= threshold).astype(int)\n",
    "    precisions.append(precision_score(y_test_new, y_pred_threshold, zero_division=1))\n",
    "    recalls.append(recall_score(y_test_new, y_pred_threshold, zero_division=0))\n",
    "    f1_scores.append(f1_score(y_test_new, y_pred_threshold, zero_division=0))\n",
    "\n",
    "# Plot precision-recall trade-off\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(thresholds, precisions, 'b-', label='Precision')\n",
    "plt.plot(thresholds, recalls, 'r-', label='Recall')\n",
    "plt.plot(thresholds, f1_scores, 'g-', label='F1-Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision, Recall, and F1-Score vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recalls, precisions, 'b-')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Choosing the Right Metric\n",
    "\n",
    "The choice of evaluation metric depends on the specific problem and the relative costs of different types of errors:\n",
    "\n",
    "- **Accuracy**: Use when classes are balanced and all types of errors have similar costs\n",
    "- **Precision**: Use when false positives are more costly (e.g., spam detection)\n",
    "- **Recall**: Use when false negatives are more costly (e.g., disease detection)\n",
    "- **F1-Score**: Use when you need a balance between precision and recall\n",
    "\n",
    "### 2.8 Metrics for Multi-class Classification\n",
    "\n",
    "For multi-class problems, these metrics can be extended using different averaging strategies:\n",
    "\n",
    "- **Macro-averaging**: Calculate the metric for each class and take the average (treats all classes equally)\n",
    "- **Micro-averaging**: Calculate the metric using the total true positives, false positives, etc. (biased toward larger classes)\n",
    "- **Weighted-averaging**: Calculate the metric for each class and take the weighted average based on class frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Multi-class metrics example\n",
    "# Using the original multi-class dataset\n",
    "knn_multi = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_multi.fit(X_train, y_train)\n",
    "y_pred_multi = knn_multi.predict(X_test)\n",
    "\n",
    "# Calculate metrics with different averaging strategies\n",
    "precision_macro = precision_score(y_test, y_pred_multi, average='macro')\n",
    "precision_micro = precision_score(y_test, y_pred_multi, average='micro')\n",
    "precision_weighted = precision_score(y_test, y_pred_multi, average='weighted')\n",
    "\n",
    "recall_macro = recall_score(y_test, y_pred_multi, average='macro')\n",
    "recall_micro = recall_score(y_test, y_pred_multi, average='micro')\n",
    "recall_weighted = recall_score(y_test, y_pred_multi, average='weighted')\n",
    "\n",
    "f1_macro = f1_score(y_test, y_pred_multi, average='macro')\n",
    "f1_micro = f1_score(y_test, y_pred_multi, average='micro')\n",
    "f1_weighted = f1_score(y_test, y_pred_multi, average='weighted')\n",
    "\n",
    "# Display results in a table\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Macro': [precision_macro, recall_macro, f1_macro],\n",
    "    'Micro': [precision_micro, recall_micro, f1_micro],\n",
    "    'Weighted': [precision_weighted, recall_weighted, f1_weighted]\n",
    "}, index=['Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "print(\"Multi-class Classification Metrics:\")\n",
    "print(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **k-Nearest Neighbors (kNN)**:\n",
    "   - A non-parametric, instance-based learning algorithm\n",
    "   - Uses distance metrics to find the k closest training examples\n",
    "   - Makes predictions based on majority voting\n",
    "   - The choice of k affects the complexity of decision boundaries\n",
    "\n",
    "2. **Classification Evaluation Metrics**:\n",
    "   - Confusion Matrix: A table showing the counts of true/false positives/negatives\n",
    "   - Accuracy: The proportion of correct predictions\n",
    "   - Precision: The proportion of true positives among predicted positives\n",
    "   - Recall: The proportion of true positives among actual positives\n",
    "   - F1-Score: The harmonic mean of precision and recall\n",
    "   - Precision-Recall Trade-off: Adjusting the decision threshold affects precision and recall\n",
    "\n",
    "These concepts form the foundation for understanding classification algorithms and evaluating their performance. In the next notebook, we'll explore Voronoi diagrams and their connection to kNN and Bayesian decision theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern Classification (2nd ed.). Wiley-Interscience.\n",
    "2. [Precision and Recall - Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "3. [Confusion Matrix - Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "4. [Scikit-learn Documentation - Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}